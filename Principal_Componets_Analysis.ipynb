{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Principal Componets Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhin-koshy/Notebooks/blob/master/Principal_Componets_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrlG7fJ0hlgT",
        "colab_type": "text"
      },
      "source": [
        "# Principal Components Analysis  (PCA)\n",
        "\n",
        "In this module we will look into the PCA technique for data compression and dimensionality reduction.\n",
        "\n",
        "Let $x_i$, $i = 1,2, \\ldots,M$ denote the data samples. We assume that the data samples have zero mean.  If not, we subtract the empirical mean from  each samples to make the samples zero mean, i.e.,\n",
        "$x_i = x_i - \\mu$, where $\\mu = \\frac{1}{M}\\sum_{i=1}^Mx_i$.\n",
        "\n",
        "PCA basically finds the eigenvectors of the covariance matrix $\\Sigma = \\frac{1}{M}\\sum_{i=1}^M x_i x_i^T$. The datapoints are then projected along the principal eigenvectors of the covariance matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzWkIgwLFy3h",
        "colab_type": "text"
      },
      "source": [
        "### PCA derivation for projection to single dimension\n",
        "\n",
        "We will now show that PCA provides the optimal linear projection that minimises the mean squared error between the original data points and the re-projected data points.\n",
        "\n",
        "Let us begin by looking at projections to a single dimension. For  a projection vector $w$, let the re-projection for a data point $x$ be denoted by $\\hat{x}= (w^Tx)w$. Note that all projection vectors need to be unit norm vectors, i.e., $w^Tw = 1$. The mean square error (MSE) for all the data points is\n",
        "\\begin{align}\n",
        "MSE(w) = \\frac{1}{M} \\sum_{i=1}^M \\Vert x_i - (w^Tx_i)w \\Vert^2.\n",
        "\\end{align}\n",
        "\n",
        "The aim is to find the optimum $w^*$ that minimises the MSE\n",
        "\n",
        "\\begin{align}\n",
        "w^* &= \\arg\\min_{w:\\Vert w \\Vert = 1} MSE(w)\\\\\n",
        "& =\\arg\\min_{w:\\Vert w \\Vert = 1}  \\frac{1}{M} \\sum_{i=1}^M \\Vert x_i - (w^Tx_i)w \\Vert^2\\\\\n",
        "& = \\arg\\min_{w:\\Vert w \\Vert = 1}  \\frac{1}{M} \\sum_{i=1}^M (x_i - (w^Tx_i)w)^T  (x_i - (w^Tx_i)w)\\\\\n",
        "& = \\arg\\min_{w:\\Vert w \\Vert = 1}  \\frac{1}{M} \\sum_{i=1}^M x_i^Tx_i - (w^Tx_i)(w^Tx_i) - (w^Tx_i)(x_i^Tw)+(w^Tx_i)^2 w^Tw\\\\\n",
        "& = \\arg\\min_{w:\\Vert w \\Vert = 1}  \\frac{1}{M} \\sum_{i=1}^M x_i^Tx_i - (w^Tx_i)^2\\\\\n",
        "&= \\arg\\max_{w:\\Vert w \\Vert = 1}  \\frac{1}{M} \\sum_{i=1}^M w^Tx_ix_i^Tw\\\\\n",
        "&= \\arg\\max_{w:\\Vert w \\Vert = 1}  w^T \\left(\\frac{1}{M} \\sum_{i=1}^M x_ix_i^T \\right)w.\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhHLv0HeK8au",
        "colab_type": "text"
      },
      "source": [
        "We know that the solution to the above problem is the principal eigenvector of the covariance matrix $\\frac{1}{M} \\sum_{i=1}^M x_ix_i^T $. Similarly, we can show that the projection matrix for projecting to a $k$ dimensional subspace consists of the principal $k$ eigenvectors of the covariance matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sb3NbJkhaIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_#import required modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}