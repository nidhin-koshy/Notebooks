{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhin-koshy/Notebooks/blob/master/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FpME5J7afA7",
        "colab_type": "text"
      },
      "source": [
        "#Regression\n",
        "\n",
        "Regression deals with finding an appropriate function mapping from the input variables (independednt variables) to the output variable (dependent variable) from the available data points. The aim is to obtain a regression function that minimizes the appropriate cost function, and further that it belongs to the constrained class of regression functions. \n",
        "\n",
        "##Linear Regression\n",
        "\n",
        "Regression where the class of regression functions are restricted to linear or affine functions. Linear regression is one of the easiest and simplest forms of regression.\n",
        "\n",
        "##System Model\n",
        "For sample index $i$, let  $x_i = (x_{i1}, x_{i2}, \\ldots,x_{iK})^T \\in \\mathcal{R^n}$ denote the values for input variables, and let $y_i \\in \\mathcal{R}$ denote the value for the output variable.\n",
        "\n",
        "Let $N$ denote the total number of training samples. \n",
        "\n",
        "Let $\\hat{y} := \\theta^Tx$ denote the linear regression, where $\\theta$ denotes the regression coefficients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlXQ0dKBasdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us generate synthetic data points assuming a two-dimensional input variable and an affine map between the input variable and the output variable.\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def mean_squared_error(output_var,input_var,theta):\n",
        "  error = output_var-np.matmul(input_var,theta)\n",
        "  return (np.linalg.norm(error)**2)/np.size(error)\n",
        "\n",
        "num_samples = 1000;\n",
        "noise_var = 100; #variance of additive Gaussian noise added to the output.\n",
        "theta_true = np.array([[3.2, 40.1, 400]]).transpose() # the third element corresponds to the constant component.\n",
        "input_var_range = 20 # the range of input variables will be from [-input_var_range, input_var_range]\n",
        "#print(theta)\n",
        "\n",
        "x = np.random.rand(num_samples,2)*2*input_var_range - input_var_range # generate uniformly distributed random input points\n",
        "x1 = np.concatenate((x, np.ones((num_samples,1),dtype=x.dtype)), axis=1) # append all ones column to x. This is required to estimate the bias\n",
        "\n",
        "y = np.matmul(x1,theta_true) # noiseless linear output\n",
        "noise_samples =  math.sqrt(noise_var)*np.random.randn(num_samples,1) #generate noise samples\n",
        "y1 = y + noise_samples # add noise samples to the output\n",
        "\n",
        "#print(y[1:5])\n",
        "#print(y1[1:5])\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "ax = plt.axes(projection='3d') #initialize a 3D-plot\n",
        "\n",
        "ax.plot_trisurf(x1[:,0], x1[:,1], y[:,0]) #plot the affine-plane using the noise-less output\n",
        "ax.plot(x1[:,0], x1[:,1], y1[:,0],'.r')   #plot the noisy samples\n",
        "\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$y$')\n",
        "plt.title('The affine plane ' r'$y = \\theta_1x_1 + \\theta_2x_2+b$' '\\n The red points corresponds to noisy observations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U36We78ux3Jv",
        "colab_type": "text"
      },
      "source": [
        "##Least Squares\n",
        "\n",
        "Least squares solution, or the regression coefficients, ($\\theta^*$) is defined as $$ \\theta^* = \\arg \\min_\\theta \\sum_i^N \\Vert y_i -  \\theta^Tx_i\\Vert^2$$\n",
        "\n",
        "The least square solution can be derived to be the product of Moore-Penrose pseudo inverse of $X$ and $y$\n",
        "\n",
        "$$\\theta^* = (X^TX)^{-1}X^Ty,$$\n",
        "\n",
        "where $X =  \\begin{bmatrix} x_1^T  \\\\ x_2^T\\\\\\vdots\\\\ x_n^T \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JPAbhJ4zSQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = np.linalg.inv(np.matmul(x1.transpose(),x1)) #compute inv(X^T*X)\n",
        "pinv_x1 = np.matmul(temp,x1.transpose()) # compute the pseudo inverse of x1. try the inbulit pseudo inverse function available in numpy\n",
        "theta_LS = np.matmul(pinv_x1,y1)\n",
        "print(\"LS solution = \",theta_LS)\n",
        "print(\"The mean squared error = \", mean_squared_error(y1,x1,theta_LS))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvuuo0j67tti",
        "colab_type": "text"
      },
      "source": [
        "##Gradient Descent based optimization\n",
        "\n",
        "The least squares solution is the optimum solution that minimizes the squared error. However, finding the pseudo inverse can be computationally intensive due to the need to find a matrix inverse, especially when the number of features are large. An alternative approach is to iteratively arrive at the optimum solution via gradient descent. \n",
        "\n",
        "Let $\\theta$ denote the current regression coefficients. The empirical mean squared error is given by\n",
        "\n",
        "$$\\bar{e}^2(\\theta) := \\frac{1}{N}\\sum_{i=1}^N (y_i-\\theta^Tx_i)^2$$\n",
        "\n",
        "The partial derivative with respect to $\\theta_j$ is \n",
        "$$\\frac{\\partial \\bar{e}^2(\\theta)}{\\partial \\theta_j} = \\frac{1}{N}\\sum_{i=1}^N 2*(y_i-\\theta^Tx_i) (-x_{ij}).$$\n",
        "\n",
        "The gradient can then be written as  $$\\nabla_\\theta(\\bar{e}^2(\\theta)) = -\\frac{2}{N} X^T(y-X\\theta)$$\n",
        "\n",
        "##Batch gradient descent\n",
        "In batch gradient descent, we descent in the direction opposite to the gradient descent calculated as computed above. The resulting update rule for iteration $n+1$ is \n",
        "$$\\theta_{n+1} = \\theta_n -\\mu_n \\nabla_\\theta(\\bar{e}^2(\\theta_n)),$$\n",
        "where $\\mu_n$ is known as the learning rate or stepsize at iteration $n$ of the update rule, and $\\bar{e}^2(\\theta_n)$ denotes the mean squared error associated with the regression coefficients $\\theta_n$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U4i1CQBOsrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iter = 20\n",
        "mu = 1/np.mean(np.linalg.norm(x1,axis=1)**2)\n",
        "print(\"mean of norm of x1 = \", np.mean(np.linalg.norm(x1,axis=1)) )\n",
        "theta = 5*np.random.rand(3,1)\n",
        "theta_evol = theta\n",
        "mse_evol = np.array(mean_squared_error(y1,x1,theta))\n",
        "for count in range(num_iter):\n",
        "  grad_theta = -1*np.matmul(x1.T,(y1-np.matmul(x1,theta)))/num_samples\n",
        "  theta1 = theta- mu*grad_theta\n",
        "  # print(theta, grad_theta,theta1)\n",
        "  theta = theta1\n",
        "  \n",
        "  theta_evol = np.concatenate((theta_evol,theta1),axis=1)\n",
        "  mse_evol = np.append(mse_evol,mean_squared_error(y1,x1,theta))\n",
        " \n",
        "plt.figure(figsize=(9,6))\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "\n",
        "ax.plot(theta_evol[0,:],theta_evol[1,:], theta_evol[2,:],'-*r')\n",
        "ax.plot(theta_evol[0,0:1],theta_evol[1,0:1], theta_evol[2,0:1],'b*')\n",
        "ax.plot(theta_evol[0,-1:],theta_evol[1,-1:], theta_evol[2,-1:],'g*')\n",
        "ax.set_xlabel(r'$\\theta_1$')\n",
        "ax.set_ylabel(r'$\\theta_2$')\n",
        "ax.set_zlabel(r'$\\theta_3$')\n",
        "plt.title(r'$\\theta$' ' evolution')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(mse_evol,'-g')\n",
        "plt.grid()\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.ylim((0, 10*mean_squared_error(y1,x1,theta)))\n",
        "plt.title('MSE evolution')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Batch gradient descent solution theta_BGD = \", theta)\n",
        "print(\"Final MSE = \", mean_squared_error(y1,x1,theta))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yumuh7k4NzM4",
        "colab_type": "text"
      },
      "source": [
        "## LMS algorithm\n",
        "\n",
        "In batch gradient descent, the gradient is computed taking into account the error with respect to all training samples. Hence, for each update to the $\\theta$ we need to compute the gradient taking into account all the available samples. An advantage of batch gradient descent is that the noise in the gradient computation will be reduced due to the averaging across samples. \n",
        "\n",
        "An alternative to this approach is to make updates after computing the gradient with respect to each training sample. Thus we make updates at a faster rate, but using more noisy gradient computations. This approach is also popularly known as the least mean squared (LMS) algorithm. LMS algorithm was proposed by Widrow and Hoff, 1960. LMS belongs to the class of stochastic gradient descent algorithms (the basis of most machine learning algorithms). As we will observe in the examples, the LMS converges faster than batch gradient descent. An intermediate approach is to compute gradient based on a mini-batch (small subset of samples) and make updates only on mini-batch basis.\n",
        "\n",
        "Let $e_i^2(\\theta)$ denote the error in sample $i$ associated with regression coefficients $\\theta$,\n",
        "\n",
        "$$e_i^2(\\theta) := (y_i - \\theta^Tx_i)^2.$$\n",
        "\n",
        "The LMS update rule is\n",
        "\n",
        "\\begin{align}\n",
        "\\nonumber \\theta_{n+1} &= \\theta_n - \\mu_n \\nabla_\\theta(e_i^2(\\theta_n))\\\\\n",
        " &=\\theta_n + 2\\mu_n x_n (y_n - \\theta_n^T x_n ),\n",
        "\\end{align}\n",
        "where $y_n$ and $x_n$ corresponds to the sample under consideration at iteration $n$. For  example, if the samples are processed in a round-robin fashion, $y_n = y_{\\mod(n,N)}$, and $x_n = x_{\\mod(n,N)}$. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80NUv6I7zK8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LMS\n",
        "# num_iter = 5\n",
        "mu = 1\n",
        "\n",
        "\n",
        "theta = 5*np.random.rand(3,1)\n",
        "print(\"theta start = \", theta)\n",
        "theta_evol = theta\n",
        "mse_evol = np.array(mean_squared_error(y1,x1,theta))\n",
        "iter_count = 1;\n",
        "for count in range(num_iter):\n",
        "  for count_inner in range(num_samples):\n",
        "    stepsize = (mu/iter_count)\n",
        "    iter_count = count+1\n",
        "    grad_theta = -(y1[count_inner:count_inner+1,0] - np.matmul(x1[count_inner:count_inner+1,:],theta))\n",
        "    grad_theta = x1[count_inner:count_inner+1,:].T* grad_theta/(np.linalg.norm(x1[count_inner:count_inner+1,:])**2) #normalizing the gradient with respect to x.\n",
        "    #print(grad_theta)\n",
        "    theta1 = theta- stepsize*grad_theta\n",
        "    theta_evol = np.concatenate((theta_evol,theta1),axis=1)\n",
        "    \n",
        " # print(theta, grad_theta,theta1)\n",
        "    theta = theta1\n",
        "    mse_evol = np.append(mse_evol,mean_squared_error(y1,x1,theta))\n",
        " \n",
        "plt.figure(figsize=(9,6))\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "ax.plot(theta_evol[0,:],theta_evol[1,:], theta_evol[2,:],'-r')\n",
        "ax.plot(theta_evol[0,0:1],theta_evol[1,0:1], theta_evol[2,0:1],'b*')\n",
        "ax.plot(theta_evol[0,-1:],theta_evol[1,-1:], theta_evol[2,-1:],'g*')\n",
        "\n",
        "plt.grid()\n",
        "ax.set_xlabel(r'$\\theta_1$')\n",
        "ax.set_ylabel(r'$\\theta_2$')\n",
        "ax.set_zlabel(r'$\\theta_3$')\n",
        "plt.title(r'$\\theta$' ' evolution')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(mse_evol,'-g')\n",
        "plt.grid()\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.ylim((0,10*mean_squared_error(y1,x1,theta)))\n",
        "plt.title('MSE evolution')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"LMS solution theta_LMS = \", theta)\n",
        "print(\"Final LMS MSE = \", mean_squared_error(y1,x1,theta))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7XllxNgcsbS",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression for non-linear functions\n",
        "\n",
        "Let us assume a function of the form $y = \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1x_2+ \\theta_4 x_1^2$. Can we use linear regression to estimate this functional? Yes, we can use linear regression as the funcational is still a linear combination of the different features, the features in the above case turns out to be $\\{x_1, x_2, x_1x_2, x_1^2\\}.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OYskG9Odo7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let x2 denote the feature vector.\n",
        "feature_x2 = np.concatenate((x[:,0:1],x[:,1:2],np.multiply(x[:,0:1],x[:,1:2]),np.multiply(x[:,0:1],x[:,0:1]),np.ones((x[:,0:1].size,1))),axis=1) #columns of x2 are[x1, x2, x1x2, x1^2,1]\n",
        "true_theta2 = 10*np.random.randn(5,1)\n",
        "output_y2 = np.matmul(feature_x2,true_theta2) + math.sqrt(noise_var)*np.random.randn(x[:,0:1].size,1)\n",
        "\n",
        "\n",
        "#LS solution\n",
        "theta2_LS = np.matmul(np.linalg.pinv(feature_x2),output_y2)\n",
        "print(\"True theta = \", true_theta2)\n",
        "print(\"Theta LS = \", theta2_LS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ED9kNNipKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LMS\n",
        "num_iter = 50\n",
        "mu = .1\n",
        "\n",
        "\n",
        "theta = 5*np.random.rand(true_theta2.size,1)\n",
        "print(\"theta start = \", theta)\n",
        "theta_evol = theta\n",
        "mse_evol = np.array(mean_squared_error(output_y2,feature_x2,theta))\n",
        "iter_count = 1;\n",
        "for count in range(num_iter):\n",
        "  for count_inner in range(num_samples):\n",
        "    stepsize = (mu/iter_count)\n",
        "    iter_count = count+1\n",
        "    grad_theta = -(output_y2[count_inner:count_inner+1,0] - np.matmul(feature_x2[count_inner:count_inner+1,:],theta))\n",
        "    grad_theta = feature_x2[count_inner:count_inner+1,:].T* grad_theta/(np.linalg.norm(feature_x2[count_inner:count_inner+1,:])**2) #normalizing the gradient with respect to x.\n",
        "    #print(grad_theta)\n",
        "    theta1 = theta- stepsize*grad_theta\n",
        "    theta_evol = np.concatenate((theta_evol,theta1),axis=1)\n",
        "    \n",
        " # print(theta, grad_theta,theta1)\n",
        "    theta = theta1\n",
        "    mse_evol = np.append(mse_evol,mean_squared_error(output_y2,feature_x2,theta))\n",
        " \n",
        "#plt.figure(figsize=(9,6))\n",
        "\n",
        "# ax = plt.axes(projection='3d')\n",
        "\n",
        "# ax.plot(theta_evol[0,:],theta_evol[1,:], theta_evol[2,:],'-r')\n",
        "# ax.plot(theta_evol[0,0:1],theta_evol[1,0:1], theta_evol[2,0:1],'b*')\n",
        "# ax.plot(theta_evol[0,-1:],theta_evol[1,-1:], theta_evol[2,-1:],'g*')\n",
        "\n",
        "# plt.grid()\n",
        "# ax.set_xlabel(r'$\\theta_1$')\n",
        "# ax.set_ylabel(r'$\\theta_2$')\n",
        "# ax.set_zlabel(r'$\\theta_3$')\n",
        "# plt.title(r'$\\theta$' ' evolution')\n",
        "# plt.show()\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(mse_evol,'-g')\n",
        "plt.grid()\n",
        "\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.ylim((0,10*mean_squared_error(output_y2,feature_x2,theta)))\n",
        "plt.title('MSE evolution')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"LMS solution theta_LMS = \", theta)\n",
        "print(\"Final LMS MSE = \", mean_squared_error(output_y2,feature_x2,theta))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBfGKyHbVdSc",
        "colab_type": "text"
      },
      "source": [
        "##Underfitting, Overfitting and Regularization\n",
        "\n",
        "When the number of parameters in the regression analysis are less than what is actually required, regression does not have the flexibility to actually learn the true function. On the other hand, when the number of parameters to be tuned are larger than what is actually required, there is a risk that the regression algorithm would try to closely fit the available sample points. The first scenario results in underfitting, while the second scenario results in overfitting. In the example below, we show the least squares regression for a quadratic function with different feature sets. When we restrict the regression function to polynomials of degree one, we observe underfitting. Whereas, when we allow the regression functions to be polynomials of degree greater than two, we observe overfitting. \n",
        "\n",
        "###Regularization \n",
        "\n",
        "Regularization is a technique to reduce over fitting. Regularization imposes a penalty on the regression coefficients and hence tries to drive down un-necessary coefficients. \n",
        "\n",
        "Linear regression with $L_2$ regularization is also known as Ridge regression. In Ridge regression, the cost function is of the form\n",
        "$$L(\\theta) = \\frac{1}{N}\\sum_{i=1}^N (y_i-\\theta^Tx_i)^2+ \\lambda \\Vert \\theta \\Vert_2^2,$$\n",
        "where $\\lambda$ is known as the regularization parameter. \n",
        "\n",
        "Similarly, one could do regression with $L_1$ regularization, and such a regression is also known as Lasso regression. In Lasso, the cost function is of the form\n",
        "\n",
        "$$L(\\theta) =  \\frac{1}{2N}\\sum_{i=1}^N (y_i-\\theta^Tx_i)^2 + \\lambda \\Vert \\theta \\Vert_1,$$\n",
        "where $\\lambda$ is known as the regularization parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sxMGj4qV1j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([np.linspace(-1,1,1000)]).T\n",
        "\n",
        "theta_true = np.array([[0, 5, 1, 1]]).T\n",
        "y = theta_true[0,0]*(x)**3 + theta_true[1,0]*(x)**2+ theta_true[2,0]*x+ theta_true[3,0]\n",
        "\n",
        "#print(\"x shape = \",x.shape)\n",
        "#print(\"y shape = \",y.shape)\n",
        "\n",
        "num_samples = 15\n",
        "noise_var = 1\n",
        "x_points = 2*np.random.rand(num_samples,1)-1;\n",
        "y_noisy = theta_true[0,0]*(x_points)**3 + theta_true[1,0]*(x_points)**2+theta_true[2,0]*x_points+ theta_true[3,0] + math.sqrt(noise_var)*np.random.randn(x_points.size,1);\n",
        "\n",
        "X1_points = np.concatenate((x_points,np.ones((x_points.size,1))),axis=1)\n",
        "X2_points = np.concatenate((x_points**2,x_points,np.ones((x_points.size,1))),axis=1)\n",
        "X3_points = np.concatenate((x_points**3,x_points**2,x_points,np.ones((x_points.size,1))),axis=1)\n",
        "X4_points = np.concatenate((x_points**4,x_points**3,x_points**2,x_points,np.ones((x_points.size,1))),axis=1)\n",
        "X5_points = np.concatenate((x_points**5,x_points**4,x_points**3,x_points**2,x_points,np.ones((x_points.size,1))),axis=1)\n",
        "\n",
        "LS_1 = np.matmul(np.linalg.pinv(X1_points),y_noisy)\n",
        "LS_2 = np.matmul(np.linalg.pinv(X2_points),y_noisy)\n",
        "LS_3 = np.matmul(np.linalg.pinv(X3_points),y_noisy)\n",
        "LS_4 = np.matmul(np.linalg.pinv(X4_points),y_noisy)\n",
        "LS_5 = np.matmul(np.linalg.pinv(X5_points),y_noisy)\n",
        "\n",
        "print(\"True theta = \", theta_true.T)\n",
        "print(\"LS_1 = \", LS_1.T)\n",
        "print(\"LS_2 = \", LS_2.T)\n",
        "print(\"LS_3 = \", LS_3.T)\n",
        "print(\"LS_4 = \", LS_4.T)\n",
        "print(\"LS_5 = \", LS_5.T)\n",
        "\n",
        "X1 = np.concatenate((x,np.ones((x.size,1))),axis=1)\n",
        "X2 = np.concatenate((x**2,x,np.ones((x.size,1))),axis=1)\n",
        "X3 = np.concatenate((x**3,x**2,x,np.ones((x.size,1))),axis=1)\n",
        "X4 = np.concatenate((x**4,x**3,x**2,x,np.ones((x.size,1))),axis=1)\n",
        "X5 = np.concatenate((x**5,x**4,x**3,x**2,x,np.ones((x.size,1))),axis=1)\n",
        "\n",
        "Y1 = np.matmul(X1,LS_1)\n",
        "Y2 = np.matmul(X2,LS_2)\n",
        "Y3 = np.matmul(X3,LS_3)\n",
        "Y4 = np.matmul(X4,LS_4)\n",
        "Y5 = np.matmul(X5,LS_5)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18,12))\n",
        "plt.plot(x,y,'-*b')\n",
        "plt.plot(x_points,y_noisy,'.r')\n",
        "\n",
        "plt.plot(x,Y1,'-g')\n",
        "plt.plot(x,Y2,'-k')\n",
        "plt.plot(x,Y3,'-c')\n",
        "plt.plot(x,Y4,'-m')\n",
        "plt.plot(x,Y5,'-y')\n",
        "plt.legend(('True function', 'samples points', 'L1 regression', 'L2 regression', 'L3 regression', 'L4 regression', 'L5 regression'))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHcjd1UXLzjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Regularization\n",
        "## Ridge, Lasso\n",
        "\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "lambda_regularization = .1\n",
        "\n",
        "reg = Lasso(alpha=lambda_regularization,fit_intercept=False)\n",
        "\n",
        "\n",
        "reg.fit(X1_points,y_noisy)\n",
        "L1_Ridge = reg.coef_\n",
        "\n",
        "reg.fit(X2_points,y_noisy)\n",
        "L2_Ridge = reg.coef_\n",
        "\n",
        "reg.fit(X3_points,y_noisy)\n",
        "L3_Ridge = reg.coef_\n",
        "\n",
        "reg.fit(X4_points,y_noisy)\n",
        "L4_Ridge = reg.coef_\n",
        "\n",
        "reg.fit(X5_points,y_noisy)\n",
        "L5_Ridge = reg.coef_\n",
        "\n",
        "print(\"True theta = \", theta_true.T)\n",
        "print(\"L1_Ridge = \", L1_Ridge)\n",
        "print(\"L2_Ridge = \", L2_Ridge)\n",
        "print(\"L3_Ridge = \", L3_Ridge)\n",
        "print(\"L4_Ridge = \", L4_Ridge)\n",
        "print(\"L5_Ridge = \", L5_Ridge)\n",
        "\n",
        "Y1_Ridge = np.matmul(X1,L1_Ridge.T)\n",
        "Y2_Ridge = np.matmul(X2,L2_Ridge.T)\n",
        "Y3_Ridge = np.matmul(X3,L3_Ridge.T)\n",
        "Y4_Ridge = np.matmul(X4,L4_Ridge.T)\n",
        "Y5_Ridge = np.matmul(X5,L5_Ridge.T)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18,12))\n",
        "plt.plot(x,y,'-*b')\n",
        "plt.plot(x_points,y_noisy,'.r')\n",
        "\n",
        "plt.plot(x,Y1_Ridge,'-g')\n",
        "plt.plot(x,Y2_Ridge,'-k')\n",
        "plt.plot(x,Y3_Ridge,'-c')\n",
        "plt.plot(x,Y4_Ridge,'-m')\n",
        "plt.plot(x,Y5_Ridge,'-y')\n",
        "plt.legend(('True function', 'samples points', 'L1 regression', 'L2 regression', 'L3 regression', 'L4 regression', 'L5 regression'))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfXR-aI1zIGh",
        "colab_type": "text"
      },
      "source": [
        "##Extras\n",
        "Exercises:\n",
        "\n",
        "*  Impact of stepsize on convergence\n",
        "*  How does a linear transformation of the independent variables affect performance\n",
        "* Need for regularization\n",
        "* Implement line search for faster convergence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEA_3AxFcXK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Observation as a linear trasformation\n",
        "# A = np.array([[1.0,1.0], [1.0,0.0]])\n",
        "# #A = np.array([[1.0,0.0], [0.0,1.0]])\n",
        "# print(\"A = \", A, \". theta = \", theta, \". inv(A)*theta = \", np.matmul(np.linalg.inv(A),theta[0:2,:]))\n",
        "\n",
        "# x1 = np.concatenate((np.matmul(x,A.T), np.ones((num_samples,1),dtype=x.dtype)), axis=1) #append all ones column to x\n",
        "\n",
        "\n",
        "# #Noisy observation of independent variables\n",
        "# #x2 = x+np.random.randn(num_samples,2)*2\n",
        "# #x1 = np.concatenate((x2, np.ones((num_samples,1),dtype=x.dtype)), axis=1) #append all ones column to x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}